{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pdb \n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = {1:2, 4:5}\n",
    "y = [[1,2,3,4],[4,2,3,4],[4,6,2,5],[3,4,5,6]]\n",
    "y.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FramesDataset(Dataset):\n",
    "    def text2Tensor(self, file_data):\n",
    "        #process the file data such that it's a list of lists of offset tuple in each time step\n",
    "        file_data_t = []\n",
    "        data_temp = []\n",
    "        try:\n",
    "            frame_num = file_data[0][0]\n",
    "        except IndexError:\n",
    "            print(\"index error in file:\")\n",
    "            print(file_data)\n",
    "        traj_list = []\n",
    "        frame_list = []\n",
    "        for line in file_data:\n",
    "            if frame_num != line[0]:\n",
    "                frame_num = line[0]\n",
    "                data_temp.sort(key=lambda data : data[1])\n",
    "                file_data_t.append(data_temp)\n",
    "                data_temp = [line]\n",
    "            else:    \n",
    "                data_temp.append(line)\n",
    "            #keep a traj list for all trajs\n",
    "            if line[1] not in traj_list:\n",
    "                traj_list.append(line[1])\n",
    "            if line[0] not in frame_list:\n",
    "                frame_list.append(line[0])\n",
    "        traj_list.sort()\n",
    "        frame_list.sort()  \n",
    "                        \n",
    "        #get participants in each frame\n",
    "        #@note here the elements are ped's index in the traj list\n",
    "        participants = [[] for i in range(len(file_data_t))]\n",
    "        for frame_idx, line in enumerate(file_data_t):\n",
    "            for traj_idx, traj in enumerate(traj_list):\n",
    "                in_flag = False\n",
    "                for data in line:\n",
    "                    if data[1] == traj:\n",
    "                        in_flag = True\n",
    "                        participants[frame_idx].append(traj_list.index(data[1]))\n",
    "                if not in_flag:\n",
    "                    file_data_t[frame_idx].append([frame_list[frame_idx], traj, 0., 0.])\n",
    "            file_data_t[frame_idx].sort(key=lambda data : data[1])\n",
    "\n",
    "        file_data_tensors = torch.tensor(file_data_t, device=device)\n",
    "        \n",
    "        participant_masks = []\n",
    "        for frame_idx, line in enumerate(participants):\n",
    "            participant_masks.append([[torch.tensor(1.) if i in participants[frame_idx] else torch.tensor(0.) for i in range(len(traj_list))]])\n",
    "        participant_masks = torch.tensor(participant_masks, device=device)\n",
    "        \n",
    "        return traj_list, participant_masks, file_data_tensors              \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    @func preprocess\n",
    "    @param path: relative path for the raw data\n",
    "    @note raw data~ col1: frame index, col2: traj index, (col3, col4): (y, x)\n",
    "    @return traj_list: indices for each trajactory in raw data\n",
    "            participants_masks~tensor(frame num x traj num): indicate the presence of each ped at each frame\n",
    "            file_data_tensors~tensor(frame num x traj num x 4): the position of each traj at each frame\n",
    "                                                                if not present default to (0,0)\n",
    "    '''\n",
    "    def preprocess(self, path):\n",
    "        #open the file as it is\n",
    "        file_data = []\n",
    "        with open(path, 'r') as file:\n",
    "            for line in file:\n",
    "                line_data = [int(float(data)) if i < 2 else float(data) for i, data in enumerate(line.rsplit())]\n",
    "                line_data[2], line_data[3] = line_data[3], line_data[2]\n",
    "                file_data.append(line_data)\n",
    "        file_data = sorted(file_data, key=lambda data : data[1])\n",
    "\n",
    "        traj_counts = {}\n",
    "        for line in file_data:\n",
    "            if line[1] not in traj_counts:\n",
    "                traj_counts[line[1]] = 1\n",
    "            else:\n",
    "                traj_counts[line[1]] += 1\n",
    "        underpresent = []\n",
    "        for k, v in traj_counts.items():\n",
    "            if v < 2:\n",
    "                underpresent.append(k)\n",
    "        \n",
    "        file_data = [data for idx, data in enumerate(file_data) if data[1] not in underpresent]\n",
    "        \n",
    "        file_data_sort = sorted(file_data, key=lambda data : data[0])\n",
    "        \n",
    "        traj_list, participant_masks, coord_tensors = self.text2Tensor(file_data_sort)\n",
    "        \n",
    "        #process the file data such that it contains the offsets not global coords\n",
    "        file_data_off = []\n",
    "        for i, line in enumerate(file_data):\n",
    "            if i > 0:\n",
    "                if file_data[i][1] == file_data[i-1][1]:\n",
    "                    file_data_off.append([file_data[i][0], file_data[i][1], file_data[i][2]-file_data[i-1][2], file_data[i][3]-file_data[i-1][3]])\n",
    "        file_data_off.sort(key=lambda data : data[0])        \n",
    "        \n",
    "        traj_list, participant_masks, off_tensors = self.text2Tensor(file_data_off)\n",
    "        \n",
    "        return traj_list, participant_masks, off_tensors, coord_tensors\n",
    "    \n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.traj_list, self.participant_masks, self.off_data, self.coord_data = self.preprocess(path)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.off_data)\n",
    "    \n",
    "\n",
    "    '''\n",
    "    @note (X, Y) is a (file_data[idx], frame[idx+1]) pair if a single idx is provided\n",
    "    a (frame[idx.start]2frame[idx.end], frame[idx.start+1]2frame[idx.end+1]) pair is provided\n",
    "    if a index slice is provided\n",
    "    the accompanying mask tensor follows the same rule \n",
    "    '''\n",
    "    def __getitem__(self, idx):\n",
    "        participant_mask = self.participant_masks[idx]\n",
    "        X = self.off_data[idx]\n",
    "        Z = torch.zeros(*X.shape)\n",
    "        for coords in self.coord_data:\n",
    "            if coords[0][0] == X[0][0]:\n",
    "                Z = coords\n",
    "                break\n",
    "        ret_data = {}\n",
    "        ret_data[\"seq\"] = X\n",
    "        ret_data[\"mask\"] = participant_mask\n",
    "        ret_data[\"idx\"] = idx\n",
    "        ret_data[\"coords\"] = Z\n",
    "        return ret_data\n",
    "\n",
    "    \n",
    "    def getTrajList(self):\n",
    "        return self.traj_list\n",
    "\n",
    "    \n",
    "    def getParticipants(self):\n",
    "        return self.participant_mask\n",
    "    \n",
    "    \n",
    "    def getCoordinates(self, seq):\n",
    "        #get the coord data at the time step right before the seq starts\n",
    "        before_coords = torch.empty(len(self.traj_list), 4, device=device)\n",
    "        for x in seq:\n",
    "            for i, coords in enumerate(self.coord_data):\n",
    "                if coords[0][0] == x[0][0]:\n",
    "                    before_coords = self.coord_data[i-1]\n",
    "                    break\n",
    "            break\n",
    "        ret_data = torch.reshape(before_coords, (1, before_coords.shape[0], before_coords.shape[1])) \n",
    "        #get the rest\n",
    "        for i, x in enumerate(seq):\n",
    "            for j, coords in enumerate(self.coord_data):\n",
    "                if coords[0][0] == x[0][0]:\n",
    "                    coords_reshaped = torch.reshape(coords.clone(), (1, before_coords.shape[0], before_coords.shape[1]))                     \n",
    "                    ret_data = torch.cat((ret_data, coords_reshaped), 0)\n",
    "                    break\n",
    "        return ret_data\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# D = FramesDataset(\"datasets/eth/train/biwi_hotel_train.txt\")\n",
    "# Dloader = DataLoader(D, batch_size=20)\n",
    "\n",
    "# for i, data in enumerate(Dloader):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phi(nn.Module):\n",
    "    ''' a non-linear layer'''\n",
    "    def __init__(self, dropout_prob):\n",
    "        super(Phi, self).__init__()\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.Dropout = nn.Dropout(p=dropout_prob)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.Dropout(self.ReLU(x))\n",
    "\n",
    "\n",
    "class SocialLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=20, mediate_dim=128, output_dim=2, social_dim=128, traj_num=3, dropout_prob=0.1,\n",
    "                N_size=2, grid_cell_size=0.3):\n",
    "        super(SocialLSTM, self).__init__()\n",
    "        #specify params\n",
    "        self.input_dim, self.mediate_dim, self.output_dim, self.hidden_dim = input_dim, mediate_dim, output_dim, hidden_dim\n",
    "        self.traj_num = traj_num\n",
    "        self.grid_cell_size = grid_cell_size\n",
    "        self.N_size = N_size if N_size % 2 == 0 else N_size + 1      \n",
    "        #specify embedding layers\n",
    "        self.InputEmbedding = nn.Linear(input_dim, mediate_dim)\n",
    "        self.SocialEmbedding = nn.Linear((self.N_size+1)**2*self.hidden_dim, social_dim)        \n",
    "        self.LSTMCell = nn.LSTMCell(mediate_dim+social_dim, hidden_dim)        \n",
    "        self.OutputLayer = nn.Linear(hidden_dim, output_dim)\n",
    "        self.Phi = Phi(dropout_prob=dropout_prob)\n",
    "        self.CorrNormLayer = nn.Sigmoid()\n",
    "\n",
    "\n",
    "#     def socialPooling(self, h_tm1, coords, mask):\n",
    "# #         print(\"=>\",end='', flush=True)\n",
    "#         tic = time.time()\n",
    "#         H = torch.zeros(coords.shape[0], self.N_size, self.N_size, self.hidden_dim, device=device)\n",
    "#         for i in range(coords.shape[0]):\n",
    "#             for j in range(coords.shape[0]):\n",
    "#                 if i == j or mask[i] == 0 or mask[j] == 0:\n",
    "#                     continue\n",
    "#                 #calc relative grid coord\n",
    "#                 grid_coord = ( int(((coords[j][0]-coords[i][0])).item() / self.grid_cell_size),\n",
    "#                                 int(((coords[j][1]-coords[i][1])).item() / self.grid_cell_size) )\n",
    "#                 #check if the coord is in the neighborhood\n",
    "#                 if np.abs(grid_coord[0]) <= self.N_size/2-1 and np.abs(grid_coord[1]) <= self.N_size/2-1:\n",
    "#                     #convert to positive for indexing\n",
    "#                     grid_coord = (int(grid_coord[0]+self.N_size/2), int(grid_coord[1]+self.N_size/2))\n",
    "#                     H[i][grid_coord[0]][grid_coord[1]] += h_tm1[j]\n",
    "        \n",
    "#         H = H.reshape(coords.shape[0], (self.N_size)**2*self.hidden_dim)\n",
    "#         toc = time.time()\n",
    "#         print(f\"ccc {toc-tic}\")\n",
    "#         return H\n",
    "    \n",
    "    def socialPooling(self, h_tm1, coords, mask):\n",
    "        with torch.no_grad():\n",
    "            H = torch.zeros(coords.shape[0], self.N_size+1, self.N_size+1, self.hidden_dim, device=device)\n",
    "            #calc margin points\n",
    "            margin_thick = 2*self.N_size*self.grid_cell_size\n",
    "            leftmost = torch.min(coords[:,0])-margin_thick\n",
    "            rightmost = torch.max(coords[:,0])+margin_thick\n",
    "            topmost = torch.min(coords[:,1])-margin_thick\n",
    "            bottommost = torch.max(coords[:,1])+margin_thick\n",
    "            ltcorner = torch.tensor([leftmost, topmost], device=device)\n",
    "\n",
    "            #calc global grid coords\n",
    "            POS = [[int(xoy) for xoy in (coords[traj_idx]-ltcorner)//self.grid_cell_size]\n",
    "                    if mask[traj_idx] != 0 else [0,0] for traj_idx in range(coords.shape[0])]\n",
    "            h_tm1_masked = mask.clone().view(mask.shape[0],1).expand(mask.shape[0],self.hidden_dim) * h_tm1.clone()\n",
    "\n",
    "            #calc global htm1 matrix\n",
    "            GRID_width, GRID_height = int((rightmost-leftmost)//self.grid_cell_size), int((bottommost-topmost)//self.grid_cell_size)\n",
    "            GRID_htm1 = torch.zeros(GRID_width,GRID_height,self.hidden_dim,device=device)\n",
    "            for traj_idx in range(coords.shape[0]):\n",
    "                GRID_htm1[POS[traj_idx][0]][POS[traj_idx][1]] += h_tm1[traj_idx]\n",
    "\n",
    "            #calc H\n",
    "            for traj_idx in range(coords.shape[0]):\n",
    "                if mask[traj_idx] != 0:\n",
    "                    x, y = POS[traj_idx][0], POS[traj_idx][1]\n",
    "                    R = self.grid_cell_size*self.N_size/2\n",
    "                    fuck = GRID_htm1[int(x-R):int(x+R),int(y-R):int(y+R),:]\n",
    "                    H[traj_idx] = GRID_htm1[int(x-R):int(x+R),int(y-R):int(y+R),:]\n",
    "\n",
    "            H = H.reshape(coords.shape[0], (self.N_size+1)**2*self.hidden_dim)\n",
    "        return H    \n",
    "\n",
    "\n",
    "    def forward(self, X, coords, part_masks, all_h_t, all_c_t, Y, T_obs, T_pred):\n",
    "        outputs = torch.empty(X.shape[0], X.shape[1], self.output_dim, device=device)\n",
    "        #array of abs coords\n",
    "        #get the splitting points after which pred starts        \n",
    "        last_points = coords[T_obs+1,:]\n",
    "        \n",
    "        for frame_idx, (x, coord) in enumerate(zip(X[:,:,2:], coords)):      \n",
    "            if frame_idx > T_pred: \n",
    "                outputs[frame_idx] = torch.zeros(X.shape[1], self.output_dim)\n",
    "                continue\n",
    "                \n",
    "            elif frame_idx <= T_obs:      \n",
    "                #calc input embedding\n",
    "                r = self.Phi(self.InputEmbedding(x))\n",
    "                #calc social pooling embedding\n",
    "                H = self.socialPooling(all_h_t, coord, part_masks[frame_idx][0])\n",
    "                e = self.Phi(self.SocialEmbedding(H))\n",
    "                concat_embed = torch.cat((r,e), 1)\n",
    "                all_h_t, all_c_t = self.LSTMCell(concat_embed, (all_h_t, all_c_t))\n",
    "                part_mask = torch.t(part_masks[frame_idx]).expand(part_masks[frame_idx].shape[1], self.output_dim)\n",
    "                outputs[frame_idx] = self.OutputLayer(all_h_t) * part_mask\n",
    "                \n",
    "            elif frame_idx <= T_pred and frame_idx > T_obs:                \n",
    "                #get the abs coords of each traj according to the last points\n",
    "                last_offs = outputs[frame_idx-1].clone()\n",
    "                for traj_idx in range(last_points.shape[0]):\n",
    "                    last_points[traj_idx] += last_offs[traj_idx]\n",
    "                #calc input embedding\n",
    "                r = self.Phi(self.InputEmbedding(last_offs))\n",
    "                #calc social pooling embedding\n",
    "                H = self.socialPooling(all_h_t, last_points, part_masks[frame_idx][0])\n",
    "                e = self.Phi(self.SocialEmbedding(H))\n",
    "                concat_embed = torch.cat((r,e), 1)                \n",
    "                all_h_t, all_c_t = self.LSTMCell(concat_embed, (all_h_t, all_c_t))\n",
    "                part_mask = torch.t(part_masks[frame_idx]).expand(part_masks[frame_idx].shape[1], self.output_dim)\n",
    "                outputs[frame_idx] = self.OutputLayer(all_h_t) * part_mask                \n",
    "                \n",
    "\n",
    "            #dirty fix for appearance that's too short\n",
    "            if frame_idx > 3 and frame_idx > T_obs:\n",
    "                for traj_idx in torch.where(part_masks[frame_idx][0] != 0)[0]:\n",
    "                    if part_masks[frame_idx-3][0][traj_idx] == 0:\n",
    "                        outputs[frame_idx, traj_idx] = Y[frame_idx, traj_idx] \n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trajPruning(part_mask, ratio=0.6, in_tensor=None):\n",
    "    if in_tensor != None:\n",
    "        #count appearance\n",
    "        for traj in range(in_tensor.shape[1]):\n",
    "            traj_mask = part_mask[:,0,traj]\n",
    "            count = traj_mask[traj_mask!=0].shape[0]\n",
    "            if count < part_mask.shape[0]*ratio:\n",
    "                in_tensor[:,traj,:] *= 0.\n",
    "        return in_tensor\n",
    "    else:\n",
    "        new_mask = part_mask.clone()\n",
    "        #count appearance\n",
    "        for traj in range(part_mask.shape[2]):\n",
    "            traj_mask = part_mask[:,0,traj]\n",
    "            count = traj_mask[traj_mask!=0].shape[0]\n",
    "            if count < part_mask.shape[0]*ratio:\n",
    "                new_mask[:,0,traj] *= 0.        \n",
    "        return new_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def train(T_obs, T_pred, file, model=None, name=\"model\"):\n",
    "    tic = time.time()\n",
    "    print(f\"training on {file}\")    \n",
    "\n",
    "    h_dim = 1024\n",
    "    batch_size = T_pred\n",
    "\n",
    "    #try to train this\n",
    "    dataset = FramesDataset(file)    \n",
    "    #a dataloader for now not sure how to use\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    traj_num = len(dataset.getTrajList())\n",
    "    h = torch.zeros(traj_num, h_dim, device=device)\n",
    "    c = torch.zeros(traj_num, h_dim, device=device)\n",
    "\n",
    "    if model == None:\n",
    "        print(\"instantiating model\")\n",
    "        sl = SocialLSTM(hidden_dim=h_dim, mediate_dim=700, output_dim=2, social_dim=256, traj_num=traj_num)\n",
    "    else:\n",
    "        sl = model\n",
    "    sl.to(device)\n",
    "\n",
    "    #define loss & optimizer\n",
    "    criterion = nn.MSELoss(reduction=\"sum\")\n",
    "    # criterion = Gaussian2DNll\n",
    "    optimizer = torch.optim.Adagrad(sl.parameters(), weight_decay=0.0005)\n",
    "    \n",
    "    plot_data = [[] for _ in range(len(dataset) // batch_size)]\n",
    "    EPOCH = 10\n",
    "    for epoch in range(EPOCH):\n",
    "        print(f\"epoch {epoch+1}/{EPOCH}  \")\n",
    "        for batch_idx, data in enumerate(dataloader):\n",
    "            print(f\"batch {batch_idx+1}/{len(dataset) // batch_size} \", end='\\r')            \n",
    "            if batch_idx < len(dataset) // batch_size:\n",
    "                Y = data['seq'][:T_pred][:T_pred,:,2:].clone()\n",
    "                input_seq = data['seq'][:T_pred].clone()\n",
    "                part_masks = data['mask']\n",
    "                coords = data['coords']\n",
    "                with torch.autograd.set_detect_anomaly(True): \n",
    "                    #dirty truncate\n",
    "                    run_ratio = (T_obs+3)/T_pred\n",
    "                    input_seq = trajPruning(part_masks, ratio=run_ratio, in_tensor=input_seq) \n",
    "                    Y = trajPruning(part_masks, ratio=run_ratio, in_tensor=Y)     \n",
    "                    pr_masks = trajPruning(part_masks, ratio=run_ratio)                                        \n",
    "                    \n",
    "                    #forward prop\n",
    "                    output = sl(input_seq, coords[:,:,2:], pr_masks, h, c, Y, T_obs, T_pred)\n",
    "\n",
    "                    #compute loss\n",
    "                    Y_pred = output[T_obs+1:T_pred]\n",
    "                    Y_g = Y[T_obs+1:T_pred]\n",
    "\n",
    "                    cost = criterion(Y_pred, Y_g)\n",
    "\n",
    "                    if epoch % 10 == 9:\n",
    "                        print(epoch, batch_idx, cost.item())\n",
    "\n",
    "                    #save data for plotting\n",
    "                    plot_data[batch_idx].append(cost.item())\n",
    "\n",
    "                    #backward prop\n",
    "                    optimizer.zero_grad()\n",
    "                    cost.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "    toc = time.time()\n",
    "    print(f\"training consumed {toc-tic}\")\n",
    "\n",
    "    #plot the cost\n",
    "    plt.figure()\n",
    "    for data in plot_data:\n",
    "        plt.plot(np.arange(len(plot_data[0])), data)\n",
    "\n",
    "    #save the model\n",
    "    torch.save(sl, name)\n",
    "    print(f\"saved model in {name}\\n\")\n",
    "\n",
    "    return sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, T_obs, T_pred, file):\n",
    "    #try to validate this\n",
    "    h_dim = 1024\n",
    "\n",
    "    batch_size = T_pred\n",
    "\n",
    "#     dataset = FramesDataset(\"crowds_zara02.txt\")\n",
    "    dataset = FramesDataset(file)    \n",
    "    #a dataloader for now not sure how to use\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    traj_num = len(dataset.getTrajList())\n",
    "    h = torch.zeros(traj_num, h_dim, device=device)\n",
    "    c = torch.zeros(traj_num, h_dim, device=device)\n",
    "    \n",
    "    plotting_batches = np.arange(40)\n",
    "    plotting_data = []\n",
    "    avgDispErrMeans = []\n",
    "    finalDispErrMeans = []    \n",
    "    #validate the model based on the dataset\n",
    "    print(f\"validating on {file}\")\n",
    "    for batch_idx, data in enumerate(dataloader):\n",
    "        if batch_idx < len(dataset) // batch_size:\n",
    "            Y = data['seq'][:T_pred,:,2:].clone()\n",
    "            input_seq = data['seq'][:T_pred].clone()\n",
    "            part_masks = data['mask']  \n",
    "            coords = data['coords']\n",
    "\n",
    "            with torch.no_grad():\n",
    "                print(f\"batch {batch_idx+1}/{len(dataset) // batch_size}  \", end='\\r')\n",
    "                #dirty truncate\n",
    "                run_ratio = (T_obs+3)/T_pred\n",
    "                input_seq = trajPruning(part_masks, ratio=run_ratio, in_tensor=input_seq) \n",
    "                Y = trajPruning(part_masks, ratio=run_ratio, in_tensor=Y)     \n",
    "                pr_masks = trajPruning(part_masks, ratio=run_ratio)\n",
    "                \n",
    "                #forward prop\n",
    "                output = model(input_seq, coords[:,:,2:], pr_masks, h, c, Y, T_obs, T_pred)\n",
    "\n",
    "                #compute cost\n",
    "                Y_pred = output[T_obs+1:T_pred]\n",
    "                Y_g = Y[T_obs+1:T_pred]\n",
    "                #......\n",
    "\n",
    "                #get and process result                \n",
    "                Y_pred_param = Y_pred.clone()\n",
    "                coords_param = dataset.getCoordinates(data['seq']).clone()\n",
    "\n",
    "                #dirty truncate\n",
    "                coords_param, Y_pred_param = trajTruncate(coords_param, Y_pred_param, part_masks, ratio=0.6)\n",
    "\n",
    "                #save plotting data for visualization\n",
    "                if batch_idx in plotting_batches:\n",
    "#                     plotting_data.append((Y_pred, part_masks, traj_num, batch_idx, dataset.getCoordinates(data['seq']), T_obs, True))\n",
    "#                     plotting_data.append((Y_pred_param, part_masks, traj_num, batch_idx, dataset.getCoordinates(data['seq']), T_obs, False)) \n",
    "\n",
    "                    plotting_batch(Y_pred, part_masks, traj_num, batch_idx, dataset.getCoordinates(data['seq']), T_obs, True)\n",
    "                    plotting_batch(Y_pred_param, part_masks, traj_num, batch_idx, dataset.getCoordinates(data['seq']), T_obs, False)\n",
    "\n",
    "\n",
    "                if batch_idx in range(len(dataset) // batch_size-1):\n",
    "                    err = avgDispError(Y_pred_param, part_masks, traj_num, batch_idx, dataset.getCoordinates(data['seq']), T_obs)\n",
    "                    avgDispErrMeans.append(err)\n",
    "\n",
    "                if batch_idx in range(len(dataset) // batch_size-1):\n",
    "                    err = finalDispError(Y_pred_param, part_masks, traj_num, batch_idx, dataset.getCoordinates(data['seq']), T_obs)\n",
    "                    finalDispErrMeans.append(err)                \n",
    "        \n",
    "#     for d in plotting_data:\n",
    "#         plotting_batch(*d)\n",
    "        \n",
    "    print(\"total avg disp mean \", np.sum(np.array(avgDispErrMeans))/len([v for v in avgDispErrMeans if v != 0]))\n",
    "    print(\"total final disp mean \", np.sum(np.array(finalDispErrMeans))/len([v for v in finalDispErrMeans if v != 0]))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@param trajs~(frame_num of a batch x traj_num x 2)\n",
    "'''\n",
    "def plotting_batch(batch_trajs_pred_gpu, part_masks, traj_num, batch_idx, coord_data, T_obs, is_total):          \n",
    "    #reform the trajs tensor to a list of each traj's pos at each frame\n",
    "    batch_trajs_pred = batch_trajs_pred_gpu.cpu().data.numpy()\n",
    "    trajs_pred_list = [np.array([]) for _ in range(traj_num)]\n",
    "    parts = []\n",
    "    for frame_idx, trajs_pred in enumerate(batch_trajs_pred):\n",
    "        for traj_idx, pos_pred in enumerate(trajs_pred):\n",
    "            if not (pos_pred == np.array([0., 0.])).all():\n",
    "                if traj_idx not in parts:\n",
    "                    parts.append(traj_idx)\n",
    "                    trajs_pred_list[int(traj_idx)] = np.array(pos_pred)  \n",
    "                else:\n",
    "                    trajs_pred_list[int(traj_idx)] = np.vstack((trajs_pred_list[int(traj_idx)], pos_pred))\n",
    "            \n",
    "    #calc the coords of each step for plotting\n",
    "    batch_coords = coord_data.cpu().data.numpy()\n",
    "    split_points = np.array(batch_coords[T_obs+1,:,2:])    \n",
    "    trajs_pred_coords = []\n",
    "    for traj_idx, traj in enumerate(trajs_pred_list):\n",
    "        traj_pred_coord = np.array(split_points[traj_idx])\n",
    "        temp_point = np.array(split_points[traj_idx])\n",
    "        for off in traj:\n",
    "            next_point = temp_point + off\n",
    "            traj_pred_coord = np.vstack((traj_pred_coord, next_point))\n",
    "            temp_point = next_point\n",
    "        trajs_pred_coords.append(traj_pred_coord)\n",
    "        \n",
    "    #plot\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plot_idx = 0\n",
    "    for traj_idx in parts:\n",
    "        try:\n",
    "            pred_x = trajs_pred_coords[traj_idx][:,0]\n",
    "        except IndexError:\n",
    "            print(\"not enough appearance\")\n",
    "            continue\n",
    "        pred_y = trajs_pred_coords[traj_idx][:,1]            \n",
    "        plt.plot(pred_x, pred_y, label=\"pred\"+str(traj_idx), marker=\".\")\n",
    "        for i, (x, y) in enumerate(zip(pred_x, pred_y)):\n",
    "            if i < len(pred_x)-1:\n",
    "                try:\n",
    "                    plt.arrow(x, y, (pred_x[i+1] - x)/2, (pred_y[i+1] - y)/2, width=0.0001, head_width=0.001, head_length=0.001)    \n",
    "                except IndexError:\n",
    "                    print(\"plot error\")\n",
    "\n",
    "        total_x = batch_coords[:,traj_idx,2]        \n",
    "        total_x = total_x[np.where(total_x != 0.)]\n",
    "        total_y = batch_coords[:,traj_idx,3]\n",
    "        total_y = total_y[np.where(total_y != 0.)]       \n",
    "        try:\n",
    "            plt.plot(total_x, total_y, linestyle=\"dashed\", label=\"total\"+str(traj_idx), marker=\".\")\n",
    "        except ValueError:\n",
    "            print(\"plot error\")\n",
    "            \n",
    "        for i, (x, y) in enumerate(zip(total_x, total_y)):\n",
    "            if i < len(total_x)-1:\n",
    "                try:\n",
    "                    plt.arrow(x, y, (total_x[i+1] - x)/2, (total_y[i+1] - y)/2, width=0.0001, head_width=0.001, head_length=0.001)\n",
    "                except IndexError:\n",
    "                    print(\"plot error\")\n",
    "        plot_idx += 1\n",
    " \n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title(f\"batch {batch_idx}\")\n",
    "    plt.savefig(\"eth_plots_social/\"+str(batch_idx)+str(is_total))    \n",
    "    \n",
    "    \n",
    "def avgDispError(batch_trajs_pred_gpu, part_masks, traj_num, batch_idx, coord_data, T_obs):\n",
    "    #reform the trajs tensor to a list of each traj's pos at each frame\n",
    "    batch_trajs_pred = batch_trajs_pred_gpu.cpu().data.numpy()\n",
    "    trajs_pred_list = [np.array([]) for _ in range(traj_num)]\n",
    "    parts = []\n",
    "    for frame_idx, trajs_pred in enumerate(batch_trajs_pred):\n",
    "        for traj_idx, pos_pred in enumerate(trajs_pred):\n",
    "            if not (pos_pred == np.array([0., 0.])).all():\n",
    "                if traj_idx not in parts:\n",
    "                    parts.append(traj_idx)\n",
    "                    trajs_pred_list[int(traj_idx)] = np.array(pos_pred)  \n",
    "                else:\n",
    "                    trajs_pred_list[int(traj_idx)] = np.vstack((trajs_pred_list[int(traj_idx)], pos_pred))\n",
    "            \n",
    "    #calc the coords of each step\n",
    "    batch_coords = coord_data.cpu().data.numpy()\n",
    "    split_points = np.array(batch_coords[T_obs+1,:,2:])    \n",
    "    trajs_pred_coords = []\n",
    "    for traj_idx, traj in enumerate(trajs_pred_list):\n",
    "        traj_pred_coord = np.array(split_points[traj_idx])\n",
    "        temp_point = np.array(split_points[traj_idx])\n",
    "        for off in traj:\n",
    "            next_point = temp_point + off\n",
    "            traj_pred_coord = np.vstack((traj_pred_coord, next_point))\n",
    "            temp_point = next_point\n",
    "        trajs_pred_coords.append(traj_pred_coord)   \n",
    "        \n",
    "    \n",
    "    #compare\n",
    "    trajs_dist = []\n",
    "    step_num = 0\n",
    "    for traj_idx, traj in enumerate(parts):\n",
    "        T_valid = T_obs+1+trajs_pred_coords[traj].shape[0]\n",
    "        diff = batch_coords[T_obs+1:T_valid,traj,2:] - trajs_pred_coords[traj]\n",
    "        traj_dist = np.sum(np.linalg.norm(diff, axis=1))        \n",
    "        if not math.isnan(traj_dist):\n",
    "            trajs_dist.append(traj_dist)\n",
    "            step_num += diff.shape[0]\n",
    "\n",
    "    if len(trajs_dist) != 0:\n",
    "        mean_of_trajs_mean = np.sum(np.array(trajs_dist))/step_num\n",
    "    else:\n",
    "        mean_of_trajs_mean = 0\n",
    " \n",
    "    print(f\"avgDispError {mean_of_trajs_mean}\")\n",
    "    return mean_of_trajs_mean\n",
    "\n",
    "\n",
    "def finalDispError(batch_trajs_pred_gpu, part_masks, traj_num, batch_idx, coord_data, T_obs):\n",
    "    #reform the trajs tensor to a list of each traj's pos at each frame\n",
    "    batch_trajs_pred = batch_trajs_pred_gpu.cpu().data.numpy()\n",
    "    trajs_pred_list = [np.array([]) for _ in range(traj_num)]\n",
    "    parts = []\n",
    "    for frame_idx, trajs_pred in enumerate(batch_trajs_pred):\n",
    "        for traj_idx, pos_pred in enumerate(trajs_pred):\n",
    "            if not (pos_pred == np.array([0., 0.])).all():\n",
    "                if traj_idx not in parts:\n",
    "                    parts.append(traj_idx)\n",
    "                    trajs_pred_list[int(traj_idx)] = np.array(pos_pred)  \n",
    "                else:\n",
    "                    trajs_pred_list[int(traj_idx)] = np.vstack((trajs_pred_list[int(traj_idx)], pos_pred))\n",
    "            \n",
    "    #calc the coords of each step\n",
    "    batch_coords = coord_data.cpu().data.numpy()\n",
    "    split_points = np.array(batch_coords[T_obs+1,:,2:])    \n",
    "    trajs_pred_coords = []\n",
    "    for traj_idx, traj in enumerate(trajs_pred_list):\n",
    "        traj_pred_coord = np.array(split_points[traj_idx])\n",
    "        temp_point = np.array(split_points[traj_idx])\n",
    "        for off in traj:\n",
    "            next_point = temp_point + off\n",
    "            traj_pred_coord = np.vstack((traj_pred_coord, next_point))\n",
    "            temp_point = next_point\n",
    "        trajs_pred_coords.append(traj_pred_coord)   \n",
    "        \n",
    "    \n",
    "    #compare\n",
    "    trajs_dist = []\n",
    "    for traj_idx, traj in enumerate(parts):\n",
    "        T_valid = T_obs+1+trajs_pred_coords[traj].shape[0]\n",
    "        diff = batch_coords[T_obs+1:T_valid,traj,2:] - trajs_pred_coords[traj]\n",
    "        final_diff = diff[-1]\n",
    "        traj_dist = np.linalg.norm(final_diff)       \n",
    "        if not math.isnan(traj_dist):\n",
    "            trajs_dist.append(traj_dist)\n",
    "\n",
    "    if len(trajs_dist) != 0:\n",
    "        mean_of_trajs_final = np.mean(np.array(trajs_dist))\n",
    "    else:\n",
    "        mean_of_trajs_final = 0\n",
    " \n",
    "    print(f\"finalDispError {mean_of_trajs_final}\")\n",
    "    return mean_of_trajs_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda:0\n",
      "\n",
      "pulling from dir datasets/eth/train\n",
      "training on datasets/eth/train/crowds_zara02_train.txt\n",
      "instantiating model\n",
      "epoch 1/10  \n",
      "batch 2/41 \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 92.00 MiB (GPU 0; 1.95 GiB total capacity; 372.27 MiB already allocated; 86.38 MiB free; 432.00 MiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a711b7b51ca2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m#training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0msl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"eth_slnew.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-79596f47c0dc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(T_obs, T_pred, file, model, name)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0;31m#forward prop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpr_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                     \u001b[0;31m#compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a27cd94fb392>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X, coords, part_masks, all_h_t, all_c_t, Y, T_obs, T_pred)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPhi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInputEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_offs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m#calc social pooling embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocialPooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_h_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpart_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mframe_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m                 \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPhi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSocialEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mconcat_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a27cd94fb392>\u001b[0m in \u001b[0;36msocialPooling\u001b[0;34m(self, h_tm1, coords, mask)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m#calc global htm1 matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mGRID_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRID_height\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrightmost\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mleftmost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_cell_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottommost\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtopmost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_cell_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mGRID_htm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGRID_width\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mGRID_height\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtraj_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mGRID_htm1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPOS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraj_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPOS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraj_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mh_tm1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraj_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 0; 1.95 GiB total capacity; 372.27 MiB already allocated; 86.38 MiB free; 432.00 MiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"device {device}\\n\")\n",
    "\n",
    "#     train_datasets = [\"datasets/eth/train\",\n",
    "#                       \"datasets/hotel/train\",               \n",
    "#                       \"datasets/univ/train\",\n",
    "#                       \"datasets/zara1/train\",\n",
    "#                       \"datasets/zara2/train\"\n",
    "#                      ]\n",
    "#     val_datasets = [\"datasets/eth/test\",\n",
    "#                     \"datasets/hotel/test\",               \n",
    "#                     \"datasets/univ/test\",\n",
    "#                     \"datasets/zara1/test\",\n",
    "#                     \"datasets/zara2/test\"\n",
    "#                     ]\n",
    "#     names = [\"eth_sl.pt\",\n",
    "#              \"hotel_sl.pt\",\n",
    "#              \"univ_sl.pt\",\n",
    "#              \"zara1_sl.pt\",\n",
    "#              \"zara2_sl.pt\"\n",
    "#             ]\n",
    "    \n",
    "#     for train_dataset, val_dataset, name in zip(train_datasets, val_datasets, names):\n",
    "#         #preparing training set\n",
    "#         files_dir = train_dataset\n",
    "#         print(f\"pulling from dir {files_dir}\")\n",
    "#         files = [join(files_dir, f) for f in listdir(files_dir) if isfile(join(files_dir, f))]\n",
    "#         sl = None\n",
    "#         #training\n",
    "#         for file in files:\n",
    "#             sl = train(8, 20, file, model=sl, name=name)\n",
    "\n",
    "#         sl1 = torch.load(name)\n",
    "#         print(f\"loading from {name}\")\n",
    "#     #     validate(vl1, 8, 20, \"try_dataset.txt\")       \n",
    "#         #preparing validating set\n",
    "#         files_dir = val_dataset\n",
    "#         print(f\"pulling from dir {files_dir}\")        \n",
    "#         files = [join(files_dir, f) for f in listdir(files_dir) if isfile(join(files_dir, f))]\n",
    "#         #validating\n",
    "#         for file in files:\n",
    "#             validate(sl1, 8, 20, file)   \n",
    "            \n",
    "#         print(\"====================================\")\n",
    "\n",
    "#     temp = train(8, 20, \"try_dataset1.txt\")\n",
    "#     validate(temp, 8, 20, \"try_dataset1.txt\")\n",
    "\n",
    "\n",
    "    #preparing training set\n",
    "    files_dir = \"datasets/eth/train\"\n",
    "    name = \"eth_slnew.pt\"\n",
    "    print(f\"pulling from dir {files_dir}\")\n",
    "    files = [join(files_dir, f) for f in listdir(files_dir) if isfile(join(files_dir, f))]\n",
    "    sl = None\n",
    "    #training\n",
    "    for file in files:\n",
    "        sl = train(8, 20, file, model=sl, name=name)\n",
    "\n",
    "    sl1 = torch.load(\"eth_slnew.pt\")\n",
    "    print(f\"loading from eth_slnew.pt\")\n",
    "#     validate(vl1, 8, 20, \"try_dataset.txt\")       \n",
    "    #preparing validating set\n",
    "    files_dir = \"datasets/eth/test\"\n",
    "    print(f\"pulling from dir {files_dir}\")        \n",
    "    files = [join(files_dir, f) for f in listdir(files_dir) if isfile(join(files_dir, f))]\n",
    "    #validating\n",
    "    for file in files:\n",
    "        validate(sl1, 8, 20, file)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gaussian2D(params, y):\n",
    "    (mu_x,mu_y), (sig_x,sig_y), rho_xy = (params[0],params[1]), (params[2],params[3]), params[4]\n",
    "    covariance = rho_xy*sig_x*sig_y\n",
    "    rv = MultivariateNormal(torch.Tensor([mu_x, mu_y]), torch.Tensor([[sig_x, covariance], [covariance, sig_y]]))\n",
    "    logP = rv.log_prob(y)\n",
    "    print(\"(mu_x,mu_y)\", (mu_x,mu_y))\n",
    "    print(\"(sig_x,sig_y\", (sig_x,sig_y))\n",
    "    print(\"rho_xy\", rho_xy)\n",
    "    print(\"y\",y)\n",
    "    print(\"logP\", logP)\n",
    "    if logP > 1500:\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    logP.requires_grad = True\n",
    "    return logP\n",
    "\n",
    "\n",
    "def Gaussian2DNll(all_params, targets):\n",
    "    print(\"\\n==================================================================================\\n\")\n",
    "\n",
    "    traj_num = targets.shape[1]\n",
    "    T = targets.shape[0]\n",
    "\n",
    "    L = torch.zeros(traj_num)\n",
    "    for traj in range(traj_num):\n",
    "        for t in range(T):\n",
    "            print('--------------------------------------')\n",
    "            print(\"L\", traj, L[traj],'\\n')\n",
    "            L[traj] += Gaussian2D(all_params[t][traj], targets[t][traj])\n",
    "    L *= -1\n",
    "\n",
    "    cost = torch.sum(L)\n",
    "\n",
    "    return cost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit11d574a7b6b045d2a23081aa8b3640ac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
